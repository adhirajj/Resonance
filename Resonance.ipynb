{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea0e14b2b3b742648ef490ef858dd49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d488457fa49a42d98dd5325420674404",
              "IPY_MODEL_32410a614c1f4b1fb77a9b3372982908",
              "IPY_MODEL_206edc418dbf46b2b9ca3dc727b6147a"
            ],
            "layout": "IPY_MODEL_1d8a59b72935400f9f26311c1c230f5d"
          }
        },
        "d488457fa49a42d98dd5325420674404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f6235261034bcbac62ce4fc3d1d617",
            "placeholder": "​",
            "style": "IPY_MODEL_e10a50f47f50407f85030ceffc4fb877",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "32410a614c1f4b1fb77a9b3372982908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e654442619f748058e1cde8508963217",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13e0e72cd8414574a4fbb9affc869632",
            "value": 6
          }
        },
        "206edc418dbf46b2b9ca3dc727b6147a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c18d5df1d63d4e44836823e38209a2c1",
            "placeholder": "​",
            "style": "IPY_MODEL_3efedce217f0478faacd0e8396c6ce99",
            "value": " 6/6 [00:01&lt;00:00,  3.10it/s]"
          }
        },
        "1d8a59b72935400f9f26311c1c230f5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f6235261034bcbac62ce4fc3d1d617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10a50f47f50407f85030ceffc4fb877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e654442619f748058e1cde8508963217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13e0e72cd8414574a4fbb9affc869632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c18d5df1d63d4e44836823e38209a2c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3efedce217f0478faacd0e8396c6ce99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ce3c4e606c64259ac3ebee0a9800861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_013035bc13064353b73f8246cfce3385",
              "IPY_MODEL_47bf6bcd5ce14cb1bebaac55b58a12b4",
              "IPY_MODEL_b1561339dee64fd891d1e09c071e6f0e"
            ],
            "layout": "IPY_MODEL_9de9a84c4c2b4a75addf1882bfa2b280"
          }
        },
        "013035bc13064353b73f8246cfce3385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9357840027404a46acc58e70e1cbf3d1",
            "placeholder": "​",
            "style": "IPY_MODEL_d23d148844484f3c9ea8938161e7b180",
            "value": "100%"
          }
        },
        "47bf6bcd5ce14cb1bebaac55b58a12b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0d374367ab4837bef9eb7f07442ad5",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee2d93a644f14ff0a23c8afd00c0f73b",
            "value": 20
          }
        },
        "b1561339dee64fd891d1e09c071e6f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfb342ec129e47969d63a1f1d55946e7",
            "placeholder": "​",
            "style": "IPY_MODEL_f03b1d7af1f64fcc8f862c7f94486f8b",
            "value": " 20/20 [00:45&lt;00:00,  2.37s/it]"
          }
        },
        "9de9a84c4c2b4a75addf1882bfa2b280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9357840027404a46acc58e70e1cbf3d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d23d148844484f3c9ea8938161e7b180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0d374367ab4837bef9eb7f07442ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee2d93a644f14ff0a23c8afd00c0f73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfb342ec129e47969d63a1f1d55946e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03b1d7af1f64fcc8f862c7f94486f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9e79ba130f6486881af87a1aa56a558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e348d94e61c444497dc078082f2962e",
              "IPY_MODEL_6abf8a46c9444b4b926efad13acd49a5",
              "IPY_MODEL_209e1d5060284cc2a351c5eb049808fc"
            ],
            "layout": "IPY_MODEL_318aa0a7f37b47fc8d89e26fce687365"
          }
        },
        "6e348d94e61c444497dc078082f2962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4d486c46c9b4914acdb58ff936be717",
            "placeholder": "​",
            "style": "IPY_MODEL_c366290b5b744894b1a3679fe3eddccc",
            "value": "100%"
          }
        },
        "6abf8a46c9444b4b926efad13acd49a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81399d8bf7c640f395d4d3f9231ccc7f",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c565c9e93604470ab6a78786f5c6ece8",
            "value": 20
          }
        },
        "209e1d5060284cc2a351c5eb049808fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18646001d74443028fdf3486e7defa61",
            "placeholder": "​",
            "style": "IPY_MODEL_e377e614e32a4277af86b7c7425f4096",
            "value": " 20/20 [00:46&lt;00:00,  2.35s/it]"
          }
        },
        "318aa0a7f37b47fc8d89e26fce687365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d486c46c9b4914acdb58ff936be717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c366290b5b744894b1a3679fe3eddccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81399d8bf7c640f395d4d3f9231ccc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c565c9e93604470ab6a78786f5c6ece8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18646001d74443028fdf3486e7defa61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e377e614e32a4277af86b7c7425f4096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PIP Installs**"
      ],
      "metadata": {
        "id": "BhFQQ84vpoNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y huggingface_hub peft diffusers transformers\n",
        "!pip install huggingface_hub peft diffusers transformers\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AhA-jo9LJbgM",
        "outputId": "56823e57-76df-44df-8326-c1da704102ea",
        "collapsed": true
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: huggingface-hub 0.28.1\n",
            "Uninstalling huggingface-hub-0.28.1:\n",
            "  Successfully uninstalled huggingface-hub-0.28.1\n",
            "Found existing installation: peft 0.14.0\n",
            "Uninstalling peft-0.14.0:\n",
            "  Successfully uninstalled peft-0.14.0\n",
            "Found existing installation: diffusers 0.32.2\n",
            "Uninstalling diffusers-0.32.2:\n",
            "  Successfully uninstalled diffusers-0.32.2\n",
            "Found existing installation: transformers 4.48.3\n",
            "Uninstalling transformers-4.48.3:\n",
            "  Successfully uninstalled transformers-4.48.3\n",
            "Collecting huggingface_hub\n",
            "  Using cached huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting peft\n",
            "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting diffusers\n",
            "  Using cached diffusers-0.32.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.1.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Using cached huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "Using cached diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
            "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "Installing collected packages: huggingface_hub, diffusers, transformers, peft\n",
            "Successfully installed diffusers-0.32.2 huggingface_hub-0.28.1 peft-0.14.0 transformers-4.48.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "diffusers",
                  "huggingface_hub",
                  "peft",
                  "transformers"
                ]
              },
              "id": "fdcbcd20882b445e851bc9c97298900a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-kgjumw6l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-kgjumw6l\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.61.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.44.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Statements**"
      ],
      "metadata": {
        "id": "nFSaCxGjpvkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dI8yZf1-JaST"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "\n",
        "# PyTorch and Diffusers\n",
        "import torch\n",
        "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Audio Processing\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import whisper\n",
        "\n",
        "# Video Processing\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
        "import imageio\n",
        "\n",
        "# GPT Integration\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting waveform bar plots and chunking**"
      ],
      "metadata": {
        "id": "qR_Fb6RJp4Mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This function manages GPU memory to prevent out-of-memory errors**"
      ],
      "metadata": {
        "id": "609TWd6O8YMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_cuda_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache() #frees unused cached memory\n",
        "        gc.collect() #triggers garbage collection\n",
        "        torch.cuda.reset_peak_memory_stats() #resest peak memory tracking stats"
      ],
      "metadata": {
        "id": "vQ1wbJkh5re-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This analyzes the audio and creates Waveform, Spectral Centroid, and Energy(RMS) plots**"
      ],
      "metadata": {
        "id": "TVDt2XHx9WWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_audio_with_plots(audio_segment, sr, segment_number):\n",
        "    \"\"\"Analyze audio and create visualization plots\"\"\"\n",
        "    # Get features\n",
        "    features = analyze_audio_emotion(audio_segment, sr)\n",
        "    emotions = interpret_features(features)\n",
        "\n",
        "    # Create subplot figure\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Waveform - Shows amplitude over time\n",
        "    plt.subplot(3, 1, 1)\n",
        "    librosa.display.waveshow(audio_segment, sr=sr)\n",
        "    plt.title(f'Waveform (Segment {segment_number})')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "\n",
        "    # Spectral Centroid - Displays frequency characteristics, indicating brightness/timbre\n",
        "    plt.subplot(3, 1, 2)\n",
        "    cent = librosa.feature.spectral_centroid(y=audio_segment, sr=sr)[0]\n",
        "    times = librosa.times_like(cent)\n",
        "    plt.plot(times, cent)\n",
        "    plt.title(f'Spectral Centroid (Brightness: {features[\"spectral_centroid_mean\"]:.2f})')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "\n",
        "    # 3. Energy (RMS) - Shows volume/intensity variations, indicating mood\n",
        "    plt.subplot(3, 1, 3)\n",
        "    rms = librosa.feature.rms(y=audio_segment)[0]\n",
        "    times = librosa.times_like(rms)\n",
        "    plt.plot(times, rms)\n",
        "    plt.title(f'Energy (RMS) - Level: {features[\"energy_mean\"]:.4f}, Mood: {emotions[\"arousal\"]}')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Energy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'segment_{segment_number}_analysis.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "383xFAzU68Pp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing emotions behind the audio**"
      ],
      "metadata": {
        "id": "07jqV5EIqECp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using RMS, Pitch, Tempo, Spectral Centroid, and Zero Crossing Rate (for voice texture) this function extracts key audio features.\n",
        "\n",
        "def analyze_audio_emotion(audio_segment, sr):\n",
        "    features = {}\n",
        "\n",
        "    # Energy and Intensity Features\n",
        "    rms = librosa.feature.rms(y=audio_segment)[0]\n",
        "    features['energy_mean'] = float(np.mean(rms))\n",
        "\n",
        "    # Pitch Features\n",
        "    pitches, magnitudes = librosa.piptrack(y=audio_segment, sr=sr)\n",
        "    pitch_means = []\n",
        "    for i in range(pitches.shape[1]):\n",
        "        pitches_t = pitches[:, i]\n",
        "        mags_t = magnitudes[:, i]\n",
        "        if len(mags_t) > 0:\n",
        "            pitch_means.append(pitches_t[mags_t == mags_t.max()][0])\n",
        "\n",
        "    if pitch_means:\n",
        "        features['pitch_mean'] = float(np.mean(pitch_means))\n",
        "\n",
        "    # Rhythm Features\n",
        "    tempo, _ = librosa.beat.beat_track(y=audio_segment, sr=sr)\n",
        "    features['tempo'] = float(np.mean(tempo))\n",
        "\n",
        "    # Spectral Features\n",
        "    cent = librosa.feature.spectral_centroid(y=audio_segment, sr=sr)[0]\n",
        "    features['spectral_centroid_mean'] = float(np.mean(cent))\n",
        "\n",
        "    # Zero Crossing Rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio_segment)[0]\n",
        "    features['zero_crossing_rate'] = float(np.mean(zcr))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "#Using the audio features extracted in the analyze_audio_emotion() function this function converts those numberical values into equivalent emotional characteristics\n",
        "\n",
        "def interpret_features(features):\n",
        "    \"\"\"Interpret the extracted features to determine emotional characteristics\"\"\"\n",
        "    emotions = {}\n",
        "\n",
        "    # Analyze energy (arousal level)\n",
        "    energy_level = features['energy_mean']\n",
        "    if energy_level > 0.1:\n",
        "        emotions['arousal'] = 'high'\n",
        "    elif energy_level > 0.05:\n",
        "        emotions['arousal'] = 'medium'\n",
        "    else:\n",
        "        emotions['arousal'] = 'low'\n",
        "\n",
        "    # Analyze pitch (emotional intensity)\n",
        "    if 'pitch_mean' in features:\n",
        "        pitch_mean = features['pitch_mean']\n",
        "        if pitch_mean > 500:\n",
        "            emotions['pitch_emotion'] = 'excited/stressed'\n",
        "        elif pitch_mean > 300:\n",
        "            emotions['pitch_emotion'] = 'neutral/positive'\n",
        "        else:\n",
        "            emotions['pitch_emotion'] = 'calm/serious'\n",
        "    else:\n",
        "        emotions['pitch_emotion'] = 'neutral'\n",
        "\n",
        "    # Analyze tempo\n",
        "    tempo = features['tempo']\n",
        "    if tempo > 120:\n",
        "        emotions['tempo_indication'] = 'energetic/happy'\n",
        "    elif tempo > 90:\n",
        "        emotions['tempo_indication'] = 'moderate/neutral'\n",
        "    else:\n",
        "        emotions['tempo_indication'] = 'slow/calm/sad'\n",
        "\n",
        "    # Analyze spectral characteristics (timbre)\n",
        "    brightness = features['spectral_centroid_mean']\n",
        "    if brightness > 2000:\n",
        "        emotions['timbre'] = 'bright/sharp'\n",
        "    else:\n",
        "        emotions['timbre'] = 'dark/warm'\n",
        "\n",
        "    return emotions"
      ],
      "metadata": {
        "id": "DK7C3RlV5s2N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoGenerator:\n",
        "    def __init__(self, device=\"cuda\", dtype=torch.float16):\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "        self.setup_pipeline()\n",
        "\n",
        "    #Sets up the AnimateDiff and pipeline woith motion adapter\n",
        "    def setup_pipeline(self):\n",
        "        reset_cuda_memory()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.set_per_process_memory_fraction(0.6)\n",
        "\n",
        "        repo = \"ByteDance/AnimateDiff-Lightning\"\n",
        "        ckpt = \"animatediff_lightning_2step_diffusers.safetensors\"\n",
        "        base = \"emilianJR/epiCRealism\"\n",
        "\n",
        "        adapter = MotionAdapter().to(self.device, self.dtype)\n",
        "        adapter.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=self.device))\n",
        "\n",
        "        reset_cuda_memory()\n",
        "\n",
        "        self.pipe = AnimateDiffPipeline.from_pretrained(\n",
        "            base,\n",
        "            motion_adapter=adapter,\n",
        "            torch_dtype=self.dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.pipe.enable_attention_slicing(slice_size=1)\n",
        "        self.pipe.enable_vae_slicing()\n",
        "        self.pipe.enable_model_cpu_offload()\n",
        "\n",
        "        self.pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
        "            self.pipe.scheduler.config,\n",
        "            timestep_spacing=\"trailing\",\n",
        "            beta_schedule=\"linear\"\n",
        "        )\n",
        "\n",
        "        reset_cuda_memory()\n",
        "\n",
        "    # Takes a text prompt and generates a 24fps short video\n",
        "    def generate_video_segment(self, prompt: str, output_path: str):\n",
        "        try:\n",
        "            reset_cuda_memory()\n",
        "\n",
        "            output = self.pipe(\n",
        "                prompt=prompt,\n",
        "                guidance_scale=7.5,\n",
        "                num_inference_steps=20,\n",
        "                num_frames=24,\n",
        "                height=512,\n",
        "                width=512\n",
        "            )\n",
        "\n",
        "            frames = [np.array(frame) for frame in output.frames[0]]\n",
        "            del output\n",
        "            reset_cuda_memory()\n",
        "\n",
        "            imageio.mimsave(output_path, frames, fps=8)\n",
        "\n",
        "            del frames\n",
        "            reset_cuda_memory()\n",
        "\n",
        "            return True\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error generating video: {str(e)}\")\n",
        "            reset_cuda_memory()\n",
        "            return False"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_wEdPYH5VkUP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_music_video(audio_file: str, output_path: str, start_time: float = 0.0, duration: float = 18.0):\n",
        "    \"\"\"Create music video with emotional analysis-driven prompts\"\"\"\n",
        "    print(f\"Creating music video from {start_time}s to {start_time + duration}s...\")\n",
        "\n",
        "    reset_cuda_memory()\n",
        "\n",
        "    # Load audio section\n",
        "    y, sr = librosa.load(audio_file, offset=start_time, duration=duration)\n",
        "    temp_audio_section = \"temp_audio_section.wav\"\n",
        "    sf.write(temp_audio_section, y, sr)\n",
        "\n",
        "    segment_duration = 3.0\n",
        "    num_segments = int(np.ceil(duration / segment_duration))\n",
        "\n",
        "    generator = VideoGenerator()\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    video_clips = []\n",
        "    for i in range(num_segments):\n",
        "        print(f\"\\nProcessing segment {i+1}/{num_segments}\")\n",
        "        print(f\"Time range: {start_time + i*segment_duration:.2f}s - {start_time + min((i+1)*segment_duration, duration):.2f}s\")\n",
        "\n",
        "        # Extract and analyze audio segment\n",
        "        start_sample = int(i * segment_duration * sr)\n",
        "        end_sample = int(min((i + 1) * segment_duration * sr, len(y)))\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Create analysis plots\n",
        "        analyze_audio_with_plots(segment, sr, i+1)\n",
        "\n",
        "        # Analyze audio emotion\n",
        "        features = analyze_audio_emotion(segment, sr)\n",
        "        emotions = interpret_features(features)\n",
        "\n",
        "        # Save temporary audio segment for transcription\n",
        "        temp_audio = f\"temp_segment_{i}.wav\"\n",
        "        sf.write(temp_audio, segment, sr)\n",
        "\n",
        "        # Transcribe lyrics\n",
        "        try:\n",
        "            result = model.transcribe(temp_audio)\n",
        "            lyrics = result['text']\n",
        "            print(f\"Transcribed lyrics: {lyrics}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {str(e)}\")\n",
        "            lyrics = \"\"\n",
        "\n",
        "        # Create GPT-4 prompt\n",
        "        context = {\n",
        "            'mood': emotions['arousal'],\n",
        "            'emotional_tone': emotions['pitch_emotion'],\n",
        "            'pacing': emotions['tempo_indication'],\n",
        "            'atmosphere': emotions['timbre'],\n",
        "            'energy_level': f\"{features['energy_mean']:.4f}\",\n",
        "            'pitch': f\"{features.get('pitch_mean', 'N/A')} Hz\",\n",
        "            'tempo': f\"{features['tempo']:.2f} BPM\",\n",
        "            'brightness': f\"{features['spectral_centroid_mean']:.2f}\",\n",
        "            'zero_crossing_rate': f\"{features['zero_crossing_rate']:.4f}\"\n",
        "        }\n",
        "\n",
        "        different_prompts = [\n",
        "                      \"\"\"Generate an abstract prompt for video generation which is black and white, where where the scene description is based on these lyrics:\n",
        "                      \"{lyrics}\"\n",
        "\n",
        "                      Detailed Audio Analysis:\n",
        "                      1. Energy & Intensity:\n",
        "                      - Energy Level: {context['energy_level']}\n",
        "                      - Overall Mood: {context['mood']} energy\n",
        "                      - Zero Crossing Rate: {context['zero_crossing_rate']} (voice texture)\n",
        "\n",
        "                      2. Tonal Qualities:\n",
        "                      - Emotional Tone: {context['emotional_tone']}\n",
        "                      - Average Pitch: {context['pitch']}\n",
        "                      - Brightness Level: {context['brightness']}\n",
        "                      - Timbre: {context['atmosphere']}\n",
        "\n",
        "                      3. Rhythmic Elements:\n",
        "                      - Tempo: {context['tempo']}\n",
        "                      - Pacing: {context['pacing']}\n",
        "\n",
        "                      Create a cinematic scene that precisely matches:\n",
        "                      - The emotional intensity indicated by the energy levels\n",
        "                      - The mood suggested by the pitch and timbre\n",
        "                      - The pacing implied by the tempo\n",
        "                      - The visual atmosphere that complements these audio characteristics\n",
        "                      - The narrative conveyed by the lyrics\n",
        "\n",
        "                      Provide specific details about:\n",
        "                      1. Visual tone (lighting, color palette)\n",
        "                      2. Camera movements\n",
        "                      3. Scene composition\n",
        "                      4. Key visual elements\n",
        "                      5. Transitions and effects\n",
        "\n",
        "                      Overall, still keep the prompt abstract and concise without losing any crucial information.\"\"\"\n",
        "\n",
        "                      ,\n",
        "\n",
        "\n",
        "                      \"\"\"Generate a concise prompt for video generation where the scene description is based on these lyrics:\n",
        "                      \"{lyrics}\"\n",
        "\n",
        "                      Detailed Audio Analysis:\n",
        "                      1. Energy & Intensity:\n",
        "                      - Energy Level: {context['energy_level']}\n",
        "                      - Overall Mood: {context['mood']} energy\n",
        "                      - Zero Crossing Rate: {context['zero_crossing_rate']} (voice texture)\n",
        "\n",
        "                      2. Tonal Qualities:\n",
        "                      - Emotional Tone: {context['emotional_tone']}\n",
        "                      - Average Pitch: {context['pitch']}\n",
        "                      - Brightness Level: {context['brightness']}\n",
        "                      - Timbre: {context['atmosphere']}\n",
        "\n",
        "                      3. Rhythmic Elements:\n",
        "                      - Tempo: {context['tempo']}\n",
        "                      - Pacing: {context['pacing']}\n",
        "\n",
        "                      Create a cinematic scene that precisely matches:\n",
        "                      - The emotional intensity indicated by the energy levels\n",
        "                      - The mood suggested by the pitch and timbre\n",
        "                      - The pacing implied by the tempo\n",
        "                      - The visual atmosphere that complements these audio characteristics\n",
        "                      - The narrative conveyed by the lyrics\n",
        "\n",
        "                      Provide specific details about:\n",
        "                      1. Visual tone (lighting, color palette)\n",
        "                      2. Camera movements\n",
        "                      3. Scene composition\n",
        "                      4. Key visual elements\n",
        "                      5. Transitions and effects\n",
        "\n",
        "                      Overall, still keep the prompt concise without losing any crucial information.\"\"\"]\n",
        "\n",
        "\n",
        "        gpt_prompt = f\"\"\"{different_prompts[0]}\"\"\"\n",
        "\n",
        "        # Get GPT response\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": gpt_prompt}]\n",
        "        )\n",
        "        prompt = response.choices[0].message.content\n",
        "\n",
        "        print(f\"Generated prompt: {prompt}\")\n",
        "        print(f\"Emotional analysis: {emotions}\")\n",
        "\n",
        "        # Generate video\n",
        "        temp_video = f\"temp_video_{i}.mp4\"\n",
        "        success = generator.generate_video_segment(prompt, temp_video)\n",
        "\n",
        "        if success:\n",
        "            clip = VideoFileClip(temp_video)\n",
        "            segment_length = (end_sample - start_sample) / sr\n",
        "            clip = clip.set_duration(segment_length)\n",
        "            video_clips.append(clip)\n",
        "\n",
        "        reset_cuda_memory()\n",
        "\n",
        "    if video_clips:\n",
        "        print(\"\\nCombining video clips and adding audio...\")\n",
        "        final_video = concatenate_videoclips(video_clips, method=\"compose\")\n",
        "        audio = AudioFileClip(temp_audio_section)\n",
        "        final_video = final_video.set_audio(audio)\n",
        "\n",
        "        final_video.write_videofile(output_path, fps=30, audio_codec='aac')\n",
        "\n",
        "        # Cleanup\n",
        "        for clip in video_clips:\n",
        "            clip.close()\n",
        "        audio.close()\n",
        "        final_video.close()\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    for i in range(num_segments):\n",
        "        for prefix in ['temp_segment_', 'temp_video_']:\n",
        "            temp_file = f\"{prefix}{i}.wav\" if 'segment' in prefix else f\"{prefix}{i}.mp4\"\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "    if os.path.exists(temp_audio_section):\n",
        "        os.remove(temp_audio_section)\n",
        "\n",
        "    reset_cuda_memory()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    reset_cuda_memory()\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    OPENAI_API_KEY = \"\"  # Replace with your key\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    create_music_video(\n",
        "        audio_file=\"audio3.mp3\",\n",
        "        output_path=\"short_music_video_1.mp4\",\n",
        "        start_time=42.0,\n",
        "        duration=6.0\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea0e14b2b3b742648ef490ef858dd49b",
            "d488457fa49a42d98dd5325420674404",
            "32410a614c1f4b1fb77a9b3372982908",
            "206edc418dbf46b2b9ca3dc727b6147a",
            "1d8a59b72935400f9f26311c1c230f5d",
            "57f6235261034bcbac62ce4fc3d1d617",
            "e10a50f47f50407f85030ceffc4fb877",
            "e654442619f748058e1cde8508963217",
            "13e0e72cd8414574a4fbb9affc869632",
            "c18d5df1d63d4e44836823e38209a2c1",
            "3efedce217f0478faacd0e8396c6ce99",
            "5ce3c4e606c64259ac3ebee0a9800861",
            "013035bc13064353b73f8246cfce3385",
            "47bf6bcd5ce14cb1bebaac55b58a12b4",
            "b1561339dee64fd891d1e09c071e6f0e",
            "9de9a84c4c2b4a75addf1882bfa2b280",
            "9357840027404a46acc58e70e1cbf3d1",
            "d23d148844484f3c9ea8938161e7b180",
            "3b0d374367ab4837bef9eb7f07442ad5",
            "ee2d93a644f14ff0a23c8afd00c0f73b",
            "bfb342ec129e47969d63a1f1d55946e7",
            "f03b1d7af1f64fcc8f862c7f94486f8b",
            "d9e79ba130f6486881af87a1aa56a558",
            "6e348d94e61c444497dc078082f2962e",
            "6abf8a46c9444b4b926efad13acd49a5",
            "209e1d5060284cc2a351c5eb049808fc",
            "318aa0a7f37b47fc8d89e26fce687365",
            "b4d486c46c9b4914acdb58ff936be717",
            "c366290b5b744894b1a3679fe3eddccc",
            "81399d8bf7c640f395d4d3f9231ccc7f",
            "c565c9e93604470ab6a78786f5c6ece8",
            "18646001d74443028fdf3486e7defa61",
            "e377e614e32a4277af86b7c7425f4096"
          ]
        },
        "id": "MIQia7EN7j79",
        "outputId": "4d8c57cb-e9bc-4c82-dd7f-9902f94a57af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating music video from 42.0s to 48.0s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea0e14b2b3b742648ef490ef858dd49b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing segment 1/2\n",
            "Time range: 42.00s - 45.00s\n",
            "Transcribed lyrics: \n",
            "Generated prompt: **Abstract Prompt for Black and White Video Generation:**\n",
            "\n",
            "---\n",
            "\n",
            "**Scene Description:**\n",
            "\n",
            "In a monochrome world where shadows dance and light whispers, the ambiance unfolds—a tapestry woven from the lyrics’ narrative, dancing between softness and raw intensity.\n",
            "\n",
            "**Detailed Audio Analysis Alignment:**\n",
            "\n",
            "1. **Energy & Intensity:**\n",
            "   - **Energy Level & Mood:** Capture a symphony of contrasts—silent storms within gentle breezes, yearning framed in tranquility.\n",
            "   - **Voice Texture:** Visualize the voice as delicate ripples across a still lake, the imagery fading in and out with each breath.\n",
            "\n",
            "2. **Tonal Qualities:**\n",
            "   - **Emotional Tone:** Portray a grayscale emotional landscape, bathed in misty nostalgia, where each note is a step into the past.\n",
            "   - **Average Pitch & Brightness:** Illustrate shadows cast by faint glimmers of light, where whispers echo between chiaroscuros.\n",
            "   - **Timbre & Atmosphere:** Encapsulate the scene in a veil of ethereal mist, where each element is intangible but vividly felt.\n",
            "\n",
            "3. **Rhythmic Elements:**\n",
            "   - **Tempo & Pacing:** Employ gentle, flowing camera movements that glide seamlessly, mirroring a heart's contemplative rhythm.\n",
            "\n",
            "**Cinematic Scene Elements:**\n",
            "\n",
            "1. **Visual Tone:**\n",
            "   - Utilize high-contrast lighting to sculpt features and forms, creating a dynamic interplay of light and dark.\n",
            "   \n",
            "2. **Camera Movements:**\n",
            "   - Opt for subtle, sweeping pans and soft focus shifts that reveal hidden depths in a fleeting glance.\n",
            "\n",
            "3. **Scene Composition:**\n",
            "   - Frame scenes with an emphasis on symmetry and balance, using reflections to suggest duality and introspection.\n",
            "\n",
            "4. **Key Visual Elements:**\n",
            "   - Introduce recurring motifs—delicate feathers, cascading rain, an old ornate clock—that evoke themes of time and fleeting emotion.\n",
            "   \n",
            "5. **Transitions and Effects:**\n",
            "   - Seamlessly dissolve between scenes, allowing transitions to mimic the natural cycle of breathing, subtly underscoring the tempo.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Craft a visual journey where the grayscale palette becomes a canvas for profound emotion, echoing the lyrics’ essence and drawing viewers into a world where music and image dance in harmonious abstraction.\n",
            "Emotional analysis: {'arousal': 'low', 'pitch_emotion': 'neutral/positive', 'tempo_indication': 'moderate/neutral', 'timbre': 'dark/warm'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (461 > 77). Running this sequence through the model will result in indexing errors\n",
            "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"capture a symphony of contrasts — silent storms within gentle breezes, yearning framed in tranquility. - ** voice texture :** visualize the voice as delicate ripples across a still lake, the imagery fading in and out with each breath. 2. ** tonal qualities :** - ** emotional tone :** portray a grayscale emotional landscape, bathed in misty nostalgia, where each note is a step into the past. - ** average pitch & brightness :** illustrate shadows cast by faint glimmers of light, where whispers echo between chiaroscuros. - ** timbre & atmosphere :** encapsulate the scene in a veil of ethereal mist, where each element is intangible but vividly felt. 3. ** rhythmic elements :** - ** tempo & pacing :** employ gentle, flowing camera movements that glide seamlessly, mirroring a heart's contemplative rhythm. ** cinematic scene elements :** 1. ** visual tone :** - utilize high - contrast lighting to sculpt features and forms, creating a dynamic interplay of light and dark. 2. ** camera movements :** - opt for subtle, sweeping pans and soft focus shifts that reveal hidden depths in a fleeting glance. 3. ** scene composition :** - frame scenes with an emphasis on symmetry and balance, using reflections to suggest duality and introspection. 4. ** key visual elements :** - introduce recurring motifs — delicate feathers, cascading rain, an old ornate clock — that evoke themes of time and fleeting emotion. 5. ** transitions and effects :** - seamlessly dissolve between scenes, allowing transitions to mimic the natural cycle of breathing, subtly underscoring the tempo. ** conclusion :** craft a visual journey where the grayscale palette becomes a canvas for profound emotion, echoing the lyrics ’ essence and drawing viewers into a world where music and image dance in harmonious abstraction.\"]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ce3c4e606c64259ac3ebee0a9800861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing segment 2/2\n",
            "Time range: 45.00s - 48.00s\n",
            "Transcribed lyrics:  Hey you\n",
            "Generated prompt: **Abstract Video Generation Prompt:**\n",
            "\n",
            "---\n",
            "\n",
            "**Visual Tone:**\n",
            "Create a monochromatic landscape, oscillating between deep shadows and pale highlights to mirror the nuanced intensity of the music. The scene should exude a stark, high-contrast atmosphere, reminiscent of early film noir, reflecting the complex emotional tone.\n",
            "\n",
            "**Camera Movements:**\n",
            "Employ smooth, gradual pans and tracking shots to echo the seamless flow suggested by the tempo and pacing. Utilize slow zooms to capture and emphasize the subtleties in expression, aligning with the intricate audio texture.\n",
            "\n",
            "**Scene Composition:**\n",
            "Design a series of evocative, minimalist tableaux, where light and shadow carve out surreal geometry. Position solitary figures within vast, empty spaces, allowing their silhouettes to convey solitude and introspection, echoing the song's lyrical narrative.\n",
            "\n",
            "**Key Visual Elements:**\n",
            "Incorporate abstract motifs like rippling water or drifting smoke, whose fluidity matches the described energy levels. Let these elements transform and merge, illustrating the emotional transitions within the song.\n",
            "\n",
            "**Transitions and Effects:**\n",
            "Use cross-dissolves to signify shifts in emotional tone, and apply a subtle grain effect to underscore the timbre’s vintage and atmospheric qualities. Let the image fade to and from black, maintaining rhythmic alignment with the music’s pacing.\n",
            "\n",
            "**Overall Atmosphere:**\n",
            "Craft a dream-like, introspective visual story that resonates with the song's intensity and depth, guiding the viewer through a cinematic experience that feels both timeless and ethereal.\n",
            "Emotional analysis: {'arousal': 'low', 'pitch_emotion': 'calm/serious', 'tempo_indication': 'energetic/happy', 'timbre': 'dark/warm'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"to echo the seamless flow suggested by the tempo and pacing. utilize slow zooms to capture and emphasize the subtleties in expression, aligning with the intricate audio texture. ** scene composition :** design a series of evocative, minimalist tableaux, where light and shadow carve out surreal geometry. position solitary figures within vast, empty spaces, allowing their silhouettes to convey solitude and introspection, echoing the song's lyrical narrative. ** key visual elements :** incorporate abstract motifs like rippling water or drifting smoke, whose fluidity matches the described energy levels. let these elements transform and merge, illustrating the emotional transitions within the song. ** transitions and effects :** use cross - dissolves to signify shifts in emotional tone, and apply a subtle grain effect to underscore the timbre ’ s vintage and atmospheric qualities. let the image fade to and from black, maintaining rhythmic alignment with the music ’ s pacing. ** overall atmosphere :** craft a dream - like, introspective visual story that resonates with the song's intensity and depth, guiding the viewer through a cinematic experience that feels both timeless and ethereal.\"]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9e79ba130f6486881af87a1aa56a558"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combining video clips and adding audio...\n",
            "Moviepy - Building video short_music_video_1.mp4.\n",
            "MoviePy - Writing audio in short_music_video_1TEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video short_music_video_1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready short_music_video_1.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "clips = [\n",
        "   VideoFileClip(\"short_music_video1.mp4\"),\n",
        "   VideoFileClip(\"short_music_video2.mp4\"),\n",
        "]\n",
        "\n",
        "final_clip = concatenate_videoclips(clips)\n",
        "final_clip.write_videofile(\"combined_music_video_new.mp4\", audio_codec='aac')\n",
        "\n",
        "# Cleanup\n",
        "for clip in clips:\n",
        "   clip.close()\n",
        "final_clip.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKaHHmztkzPJ",
        "outputId": "a888dde6-75b7-410e-e5a3-5e32496692b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video combined_music_video_new.mp4.\n",
            "MoviePy - Writing audio in combined_music_video_newTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video combined_music_video_new.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready combined_music_video_new.mp4\n"
          ]
        }
      ]
    }
  ]
}